\documentclass[a4paper,12pt]{article}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\usepackage{natbib}
%opening

%\usepackage[version=3]{mhchem} % Formula subscripts using \ce{}
\usepackage{braket}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{lipsum}     % for sample text
\usepackage{upgreek}
\usepackage{graphicx}
\usepackage{chemfig}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{underscore}
\usepackage{gensymb}
\usepackage{dcolumn}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{amssymb}


\begin{document}

\section{Question 1}
The Frobenius norm is defined as
\begin{equation}
|| A||_F = \bigg( \sum_{i=1}^m \sum_{j=1}^m |a_{ij}|^2\bigg)^{\frac{1}{2}}
\end{equation}
Furthermore, from the definition of outer product, we know some matrix $C=uv^*$ for $u\in \mathbb{C}^m, v \in \mathbb{C}^n$, and $C\in \mathbb{C}^{m\times n}$ whose  elements $c_{ij} = u_iv_j$. Inserting this into the above definition yields
\begin{equation}
||C||_F =\bigg(\sum_{i=1}^m \sum_{j=1}^n |c_{ij}|^2 \bigg)^{\frac{1}{2}}=\bigg(\sum_{i=1}^m \sum_{j=1}^n |u_i |^2 |v_j|^2 \bigg)^{\frac{1}{2}}=\bigg(\sum_{i=1}^m |u_i|^2\bigg)^{\frac{1}{2}} \bigg(\sum_{j=1}^n |v_j|^2\bigg)^{\frac{1}{2}} =||u||_F ||v||_F
\end{equation}


Ergo, we have shown that $||C||_F  =||u||_F ||v||_F$



\section{Question 2 }
\subsection{part a}
To show that the Col(A) $\perp$ Nul($A^*$) for some $A \in \mathbb{C}^{m\times n}$, we first define $y\in col(A)$. This implies that $Ax=y$ for some $x\in \mathbb{C}^n$. Furthermore, we note that there exists some $v\in nul(A^*)$, implying that $A^*v = 0$. To be perpendicular, we show in the following expression that the inner product of $(y,v)$ is zero. 
\begin{equation}
y^*v = (Ax)^*v = x^* A^* v = 0
\end{equation}
where we note from the above assertions that $A^* v = 0$. Therefore, we have proven that the $Nul(A^*) \perp col(A)$.


\subsection{part b}
We wish to show that for any $v\in \mathbb{C}^m$,
$v \perp Col(A) \implies v^*(Ax)=0$ for some $x\in \mathbb{C}^n$. Taking the adjoint of this expression yields
\begin{equation}
0^*=0=\bigg( v^*(Ax)\bigg)^* \rightarrow x^*(A^*v)=0 
\end{equation}
Thus we note that $\forall x \in \mathbb{C}^n$, the above implies that $A^*v=0 \rightarrow v\in Nul(A^*)$. In other words, we have shown that $v$ being orthogonal to Col($A$) means that $v$ must be an element of the Nul($A^*$).



\subsection{part c}

Consider a matrix $X \in \mathbb{C}^{m\times m}$. Suppose the columns of this matrix serve as a basis for $Col(A)$. By the rank-nullity theorem, we know the $dim(Nul(A^*))=m-r$. So we let the columns $x_{r+1}, \cdots, x_m$ be a basis for $Nul(A^*)$. Then

\begin{equation}
X=\begin{pmatrix}
	&	&	&	&	&	&  \\
x_1	&\cdots&x_r&x_{r+1}& \cdots&x_m\\
	&	&	&	&	&	&  \\
\end{pmatrix}
\end{equation}
where we note that $X$ is invertible and the columns of $X$ are linearly independent vectors in $\mathbb{C}^m$. Therefore, the problem has a unique solution for any $ v \in \mathbb{C}^m$
\begin{equation}
Xv=v_R + v_N =\sum_{x\in Col(A)}^r x_i v_i + \sum_{x\in Nul(A^*)}^m x_i v_i
\end{equation}

\section{Question 3}
\subsection{part a}
We will first show that Col($A^*$) = Span\{ $v_1,\cdots, v_r$\}. Suppose $A^* \in \mathbb{C}^{n\times m}$ and $(A)^* = (U\Sigma V^*)^* = V\Sigma^* U^*=A^*$. This looks like
\begin{equation}
A^* =  \begin{pmatrix}
&	&\vline&		&\\
&	&\vline&		&\\
&V_1&\vline &V_2 & \\
&	&\vline&		&\\
&	&\vline&		&\\
\end{pmatrix}
\begin{pmatrix}
\sigma_1^*&	&&	\vline	&\\
&\ddots	&&	\vline	&\\
&		&\sigma_r^*&\vline & \\ \hline
&	&&	\vline	&\\
&	&&	\vline	&\\
\end{pmatrix}
\begin{pmatrix}
&	&&		&\\
&	&U_1^*&		&\\ \hline
&	&		&			& \\ 
&	&U_2^* 	&		&\\
&	&&		&\\
\end{pmatrix}
\end{equation}
where $V\in \mathbb{C}^{n\times n}, \Sigma^* \in \mathbb{C}^{n\times m}$ and $U^* \in \mathbb{C}^{m\times m}$ and the vertical lines denote the boundary for the index corresponding to the rank of $A$. We note that the columns of $U$ are a basis for $\mathbb{C}^m$, so for some $y \in \mathbb{C}^m$, $x=Uy$. So

\begin{equation}
b=A^* x = V\Sigma^* U^* x= V\Sigma^* U^* U y= V\Sigma^* y = V\begin{pmatrix}
\sigma_1^* y_1 \\
\vdots \\
\sigma_r^* y_r \\
0 \\
\vdots \\
0 \\
\end{pmatrix} = \sum_{j=1}^r v_j (\sigma_j^* y_j)
\end{equation}

Therefore, Col($A^*$) $\subseteq$ span\{$v_1,\cdots, v_m$\}. Furthermore, for each $j=1,\cdots, r$, $A^*u_j=V\Sigma^* U^*u_j=v_j \sigma_j^*$, thereby showing that span\{$v_1,\cdots, v_r$\} $\in$ Col($A^*$). In conclusion this proves that Col($A^*$) = Span\{ $v_1,\cdots, v_r$\}. 

\subsection{part b}
For the next part, we will show that Nul($A^*$) = span\{$u_{r+1},\cdots,u_m$ \}. We build off the foundations laid in the prior subsection and we note that Nul($A^*$) = \{ $A^*x=0$\}. For $x=Uy$,  we see that
\begin{equation}
A^*x=V\Sigma^* U^* (Uy) =V\Sigma^* y =0= \sum_{j=1}^r v_j (\sigma_j^* y_j) + \sum_{j=r+1}^m 0
\end{equation}
where we note that because the columns of $V_1$ are linearly independent, the only way the equality is held is if $y_1,\cdots, y_r =0$. Consequently $A^*x=0 \rightarrow x \in $ span\{$u_{r+1},\cdots, u_m $\} alluding to Nul($A^*$) $\subseteq$ span\{$ u_{r+1}, \cdots, u_m$\}. Furthermore, since $A^*u_j=V\Sigma^*U^*u_j=0$ for $j=r+1,\cdots,m$, we know Nul($A^*$)=span\{$u_{r+1},\cdots,u_m$\}

\section{Question 4}
\subsection{part a}
See attached jupyter-notebook script.

\subsection{part b}
My custom build function that computes the induced matrix 3 norm completes in 0.0006237030029296875 seconds. Perhaps a more interesting comparison is Numpy's in house matrix infinity norm and my custom built infinity norm: the former runs in 0.00011086463928222656 seconds while the ladder runs in  0.00011014938354492188 seconds. These are obviously very comparable. I suspect the induced matrix 3 norm runs comparatively slower because it is expensive to compute powers. 

\subsection{part c}
Suppose a matrix $A \in \mathbb{C}^{m\times n}$ and some vector $x \in \mathbb{C}^n$. We want to define $\alpha$ in the relation $||A||_{\infty}=\alpha || A||_3$. To begin, we will start with the definitions of the relevant vector p-norms:
\begin{equation}
||x||_3 = \bigg( \sum_{i=1}^m |x_i|^3   \bigg)^{\frac{1}{3}}
\end{equation}
and
\begin{equation}
||x||_{\infty}= \max_{1\leq i\leq m} |x_i| = x_{max}
\end{equation}

Expanding the vector 3-norm and comparing with the vector infinity-norm, we immediately see that
\begin{equation}
x_{max}\leq \bigg( |x_1|^3 + \cdots + |x_{max}|^3 + \cdots + |x_m|^3 \bigg)^{\frac{1}{3}}
\end{equation}
which implies that $||x||_{\infty}\leq ||x||_3$, where we note pure equality is obtain if dealing with the Euclidean unit vectors. 

Additionally, we note that
\begin{equation}
\bigg( \sum_{i=1}^m |x_i|^3   \bigg)^{\frac{1}{3}}   \leq \bigg( \sum_{i=1}^m |x_{max}|^3   \bigg)^{\frac{1}{3}} \rightarrow ||x||_3 \leq m^{\frac{1}{3}} ||x||_{\infty}
\end{equation}

We then use the above two relations in addition to the definition of an induced matrix norm to compare $||A||_{\infty}$ and $||A||_3$:
\begin{equation}
||A||_{\infty} =\max_{||x||=1} \frac{||Ax||_{\infty}}{||x||_{\infty}} \leq \frac{||Ax||_3}{||x||_{\infty}} \leq (m)^{\frac{1}{3}}\frac{||Ax||_3}{||x||_3}
\end{equation}

Thus, we see that $||A||_{\infty} \leq (m)^{\frac{1}{3}}||A||_3$ where $\alpha =(m)^{\frac{1}{3}} $ as stated in the problem.

\subsection{part d}
As my approximate revolves around using Euclidean unit vectors for x, the resulting induced matrix 3 norm will be a poor approximation to the true matrix 3 norm. In fact, this is such a poor approximation that none of the 10 runs verify the inequality proven in part c. 
\end{document}